---
title: Бенчмарк локальных LLM с RTX4060Ti 16gb VS RTX5060Ti 16gb
---

Захотелось мне обновить домашний комп. В основном упираюсь в скорость выхлопа больших LLM. Посовещавшись с ворохом бесплатных нейронок, понял, что на слово им верить не хочу. Положил глаз на Gigabyte GeForce Windforce RTX5060Ti 16gb MAX. Но покупать видеокарту за 51999р слепую - неохота. Взял схожую RTX5060Ti 16gb (с разницей в том, что PCI-E 4.0 x8) у друга на вечер.

## Зачем мне LLM

Для кодинга - очень редко, раз в неделю может 1-2 строчки сгенерировать, которые гуглить лень. Больше для:

- чуть более глубокого продумывания проблем,
- пруфридинга
- поиска тенденций, на которые сам внимания не обращаю
- как резиновую уточку, способную задавать уточняющие вопросы, перед тем как давать текст живым людям.

## Бенчмарк

 Написал простенький бенчмарк на bash:

1. 4 промпта разного размера
2. несколько моделей
3. для `gemma3:4b` и `gemma3:12b` несколько производных моделей с увеличенным по-умолчанию контекстом - 8к и 16к (по-умолчанию у них 4к).
4. по два запуска - холодный (с загрузкой модели в память) и горячий
5. запускаем с флагом verbose, сохраняем stdout в один файл, stderr со статистикой в другой
6. потом питоном парсим файлы со статистикой
7. собираем всё в CSV
8. делаем несколько прогонов на каждой видеокарте
9. на своей вдобавок сделал доп. прогон на другом драйвере

Вот [сырые результаты](https://docs.google.com/spreadsheets/d/1J0B7lJoNTUvEmoPE-0lxg2LZYev8pRQn_dufVPZWxCE/edit?usp=sharing)

### Разбор больших документов

В статью вытащу только отдельный кейс - разбор длинных портянок. Hello-world ведь никому не интересен, кодинг - больше интересен не с точки зрения времени, а качества результата. Всё в табличке ниже проверялось на одном и том же промпте.

> "Подведи итоги двух месяцев на основе моих записей в дневнике. Выдели самое важное и те вещи, которые стоит обдумать. Если заметишь какие-то тенденции - опиши их. Попробуй в целом оценить моё состояние на основе этих записей"

плюс сами записи, итого:

``` shell
wc prompts/22244_diary.md

- 252 строк
- 3568 слов
- 38748 символов
```

Примечания:

- Текст скорее всего не влезает в контекст 4к, так что это означает, что работа ведётся над огрызком текста.
- Надо было заморочиться, seed выставить статический для бенчмарка, но я как-то продолбался, поэтому на общее время генерации смотреть смысла мало, так, порядки оценить.

### Замеры

| Модель            | Видеокарта     | Время, сек | Скорость, токенов/сек |
| ----------------- | :------------- | ---------- | --------------------- |
| gemma3:4b         | RTX4060Ti 16gb | 11.09      | 81                    |
| gemma3:4b         | RTX5060Ti 16gb | 9.74       | 119.89                |
| gemma3:12b, 16k   | RTX4060Ti 16gb | 37         | 29.5                  |
| gemma3:12b, 16k   | RTX5060Ti 16gb | 27         | 44.1                  |
| gemma3:27b        | RTX4060Ti 16gb | 118        | 6.59                  |
| gemma3:27b        | RTX5060Ti 16gb | 108        | 7.54                  |
| qwen3:30b         | RTX4060Ti 16gb | 113        | 22.86                 |
| qwen3:30b         | RTX5060Ti 16gb | 100        | 23.79                 |
| mistral-small:24b | RTX4060Ti 16gb | 53         | 17.74                 |
| mistral-small:24b | RTX5060Ti 16gb | 50         | 26.94                 |
| ministral-3:14b   | RTX4060Ti 16gb | 149        | 9.62                  |
| ministral-3:14b   | RTX5060Ti 16gb | 205        | 10.48                 |

### Итоги

- У `gemma3:12b` выкрутил максимальный размер контекста до 16к, у остальных - стандартный 4к. У gemma3:12b прирост и по скорости и по времени прям приятный - по скорости 49%.
- У `gemma3:4b` в токенах приятнее звучит, конечно (почти 47% прирост). А фактически выиграл 1.3сек (скорее всего просто ответ получился короче).
- `mistral-small:24b,` если не расширять контекст, впритык влезает в VRAM, поэтому получил хороший прирост в скорости аж в 50%. Это много!
- `gemma3:27b` - прирост около 14%. Вроде +1 токен/сек, но для такой черепахи неплохо.
- `ministral-3:14b` вроде по размерности (14b) мелкий, но жирный и не влезает в VRAM. Прирост 8%.
- `qwen3:30b` - прирост всего 4%. А ведь это одна из основных моделей, которыми я пользуюсь.

## Выводы

Если использовать **большие модели**, даже MoE (Mixture of Experts): большого профита нет. Всё равно медленно и проще чем-то заняться, пока ответ сгенерируется.

Если в **интерактивном режиме** использовать модели типа `gemma3:12b`, то от прироста скорости нет ощутимой выгоды - читаю я всё равно медленнее.

Если использовать **модели поменьше** в автоматическом режиме и постоянно (в пайплайнах, скриптах итд) - начинается ощутимый выигрыш. `gemma3:4b` прям тоже хороша, чтобы пачками генерировать заголовки новым файлам в obsidian за секунды (например при импорте из mastodon или с диктофона).

Я упустил важную идею: не проверил, сколько памяти сжирает скармливание **фоток** мультимодальной `gemma3:12b`. Я категоризирую фотографии с её помощью. В теории, если не выходить за пределы VRAM и получить выигрыш в 30% - это прям кайф, потому что оно там молотит по 5-10 часов порой. Будет молотить по 4-7 часов. Таки выгода. Но за 51к? Хз.

Кажется, что оптимальная точка для моего дальнейшего **апгрейда**:

- Объём VRAM. Промышленные ускорители б/у на 32гб стрёмно. AMD AI TOP - дорого и не факт что влезут в мой корпус и я смогу их охладить. Ждать когда китайцы напаяют 32гб на 5060 за копейки.
- Частота оперативки. Разгонять оперативку? Полгода назад бы попробовал. Сейчас жалко, ибо она золотая.
- Есть ещё сомнительные гипотезы про CPU: типа больше ядер, б*о*льшая частота ядер, б*о*льший L3-кэш. Но проверять их муторно.
- Можно попробовать повозиться с софтом, поприбивать слои к видяхе и RAM у MoE-моделей, чтобы минимизировать шатание данных туда сюда. Вроде LMStudio для этого даёт более внчтные инструменты чем ollama. Но это уже не вопрос апгрейда.
- Ну и суперсомнительная гипотеза: RTX5060ti *max* с pci x16. Но апгрейд с pci3 на pci-e 4.0 мне толком ничего не дал. Думаю тут будет то же самое.

Идеальным решением было бы *полгода назад* перейти на ryzen 9700x + 2х64gb DDR5. Теперь - скорее ждать 3 года. Чем я и займусь.

Начал больше ценить `gemma3:12b` - она достаточно адекватна, при этом весит достаточно мало, чтобы в VRAM напихать *здоровенный* резиновый контекст.

## Чего не хватило

Жалею немного о том, что `qwen3-coder:30b` не побенчмаркал и `qwen3:32b` (если 30b сравнительно бодро работает, 32b - уже черепаха). Потом как-нибудь просто добавлю прогон на своей видеокарте в сырые данные.

Ну и надо бы код для бенчмарканья в опенсорс выложить, хотя в нём ничего особенного нет.

## Бутылочное горлышко

> Ну дык боттлнек у тебя в I/O с RAM при оффлоаде

Всё так, я просто на практике подтвердил эту гипотезу.

## Рационализация покупки

> Прикинь, сколько времени и электричества сэкономишь и сколько денег получишь от продажи старой видеокарты, прикинь насколько легче будет вайбкодить, сколько на этом заработаешь, да едь покупать.

Не могу. Я старую карту купил за 61к, не смог продать за 35к и снял объявление, потому что задавила жабка. Но ничо, ща nvidia такая - "оперативка дорогая, гейминг мёртв, продажи падают, мы тоже уходим целиком в ии, а вы там держитесь как-нибудь" - цены подрастатут, снова выложить можно. Ну или на дачу второй комп собирать буду.
